--- 
key: polo
speaker: Polo Chau
website: https://www.cc.gatech.edu/~dchau/
affiliation: Georgia Tech
title: Towards Safe and Interpretable AI
time: 
picture: polo.jpg
picture-note: Polo Chau
slides: 
bio: |
    Duen Horng (Polo) Chau is an Associate Professor of Computing at Georgia Tech. He co-directs Georgia Tech's MS Analytics program. He is the Director of Industry Relations of The Institute for Data Engineering and Science (IDEaS), and the Associate Director of Corporate Relations of The Center for Machine Learning. His research group bridges machine learning and visualization to synthesize scalable interactive tools for making sense of massive datasets, interpreting complex AI models, and solving real world problems in cybersecurity, human-centered AI, graph visualization and mining, and social good. His Ph.D. in Machine Learning from Carnegie Mellon University won CMU's Computer Science Dissertation Award, Honorable Mention. He received awards and grants from NSF, NIH, NASA, DARPA, Intel (Intel Outstanding Researcher), Google, Facebook, NVIDIA, Bosch, Amazon, Microsoft, Cisco, Symantec, eBay, Yahoo, LexisNexis; Raytheon Faculty Fellowship; Edenfield Faculty Fellowship; Outstanding Junior Faculty Award; The Lester Endowment Award; Symantec fellowship (twice); IEEE VIS'20 Best Poster Research Award, Honorable Mention; ACM TiiS 2018 Best Paper, Honorable Mention, Best student papers at SDM'14 and KDD'16 (runner-up); Best demo at SIGMOD'17 (runner-up); Chinese CHI'18 Best paper. His research led to open-sourced or deployed technologies by Intel (for ISTC-ARSA: ShapeShifter, SHIELD, ADAGIO, MLsploit), Google (GAN Lab), Facebook (ActiVis), Symantec (Polonium, AESOP protect 120M people from malware), and Atlanta Fire Rescue Department. His security and fraud detection research made headlines. He is a steering committee member of ACM IUI conference, IUI’15 co-chair, and IUI’19 program co-chair. He is an Associate Editor for IEEE TVCG. He was publicity chair for ACM KDD'14 and ACM WSDM'16 He co-organized the popular IDEA workshop (at KDD) that catalyzes cross-pollination across HCI and data mining.

abstract: |
    Tremendous growth in artificial intelligence (AI) research has shown that AI is vulnerable to adversarial attacks, and their predictions can be difficult to understand, evaluate and ultimately act upon. Our Safe AI research thrust discovers real-world AI vulnerabilities and develops countermeasures to fortify AI deployment in safety-critical settings: ShapeShifter, the world's first targeted physical attack that fools faster R-CNN object detector; the UnMask defense that flags semantic incoherence in predictions (part of DARPA GARD); the TIGER toolbox for GPU-accelerated graph vulnerability and robustness analysis (part of Nvidia Data Science Teaching Kit); MalNet, the largest public cybersecurity graph database with over 1.2M graphs (100X more). Our complementary Interpretable AI research designs and develops interactive visualizations that amplify people’s ability to understand complex models and vulnerabilities, and provide key leaps of insight: Summit, NeuroCartography, and Bluff, systems that scalably summarize and visualize what features a deep learning model has learned, how those features interact to make predictions, and how they may be exploited by attacks; SkeletonVis, the first interactive tool that visualizes attacks on human action recognition models; CNN Explainer and GAN Lab (with Google Brain), accessible viral tools for students and experts to learn about AI models. We conclude by highlighting the next visual analytics research frontiers in AI.
---